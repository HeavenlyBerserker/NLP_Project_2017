NLP Project 2017
Project by Hong Xu and Jie Zhang
Written on python 2.7.12

Created October 28 2017
Sources for nltk: http://www.nltk.org/book_1ed/
Credit to : Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. Oâ€™Reilly Media Inc.


#######################################################
Instructions so far:

1. To install python repositories and test the system on a file, run test.sh with the filename to test and answers as a single argument:

	./testFinal.sh <textfile>
		
	
2. [Optional] To start learning process only. This may take a few dozen seconds, so run at your own peril. Run project.py with no arguments. Suggested version 2.7.12
	
	python project.py

a ######################################################
External sources:
nltk
spacy

b ######################################################
Time Estimate:
	For file prediction
		<1s per file
	
	For training:
		<45s total
		
	Total for samples:
		~120s


c ######################################################
Team member contributions.

Hong Xu: Was responsible for writing the code to parse the files into usable text format. Created the system to create patterns and find triggers. Implemented a way to classify incidents by frequency of certain words. Responsible for the testing and experimentation on the test files.

Zhang Jie: Read AutoSlog paper and understand basic steps/concepts.
Tried to implement a NER system that will learn words in the categories and tried to classify new words seen. So far does not work well.
Implemented a method that extracts relatively long sentence chunks as patterns.
	
d ######################################################
Progress, problems and limitations:

	We have built a system similar to autoslog, however, we have struggled to find a way to classify words into classes (location, person, inanimate object, untangible). Without this, our precission has suffered severely.
	
	At the moment, our recall hovers between 35% and 45%, whereas our precision is low at 3% to 5%. Due to the inconsistency between the two scores, we can only achieve f-measures between 7% and 10%. In future builds, we aim to bring up the precision so it matches the recall in order to bring up our score. We have deviced various methods to classify our responses, from figuring out their class (location, person, inanimate object, untangible) to deeply examining the origins of every chosen word. Furthermore, we also hope to bring up the recall by trimming and further engineering our outputs.


Change log:
2017-10-29 Added readme file and did some modifications to the code.
2017-10-29 Parsed the file further. Now each file has been put with
			its answers, parsed answers, raw text, and parsed sentences.
2017-10-29 Added finding important words
2017-11-04 Figured out parser and created some patterns. Work in progress.
2017-11-04 Wrote a custom parser using the nltk tagger. Results are okay. Higher correct answer instances in answers.
2017-11-05 Wrote predict.py to start evaluating answers.
2017-11-05 Tried using verbs and verb phrases as trigger words. Experiments failed, so we are using any word with high P log F score.
2017-11-06 Struggled with finding better parsers, expecially in terms of separating subjects, DOs and PPs.
2017-11-06 Tried different configurations for detecting incidents by exploring the ratio between words that appear in articles vs other articles, capped at around 50% recall.
2017-11-06 With prunning of some answers, obtained a recall of 20%, but still low precision of 1%.
2017-11-06 Improved recall to ~25%.
2017-11-07 Trying to improve system to no avail.

